{"nbformat":4,"nbformat_minor":0,"metadata":{"kernel_info":{"name":"Weblab"},"language_info":{"name":"javascript"}},"cells":[{"cell_type":"markdown","metadata":{},"source":["# Face recognition pipeline with BlazeFace and ArcFace-MobileNet\n\nIn this example we want to build a modern face detection pipeline that is capable of identifying the same person in different pictures. A modern face detection pipeline typically consists of these steps:\n\n1. Face detection\n2. Face alignment\n3. Feature extraction\n4. Feature matching\n\nIn the face detection step the algorithm tries to find all faces in the given picture. In this example we will use the [BlazeFace](https://arxiv.org/abs/1907.05047) neural network to perform the detection task. At the end of the step each face gets cut out into their own image.\n\nThe next steps are performed on each face individually. The faces in the original picture may be tilted or have different sizes depending on their distance from the camera. To improve the performance each face gets straightened and resized to the same size.\n\nIn the feature extraction step the input face is mapped into a vectorial representation called an embedding. In this example we will use a [MobileNet](https://arxiv.org/abs/1704.04861) model that has been fine-tuned with the [ArcFace](https://arxiv.org/abs/1801.07698) loss function for high accurracy. The model was taken from this [Keras ArcFace](https://github.com/leondgarse/Keras_insightface) implementation and converted to the Tensorflowjs format.\n\nIn the feature matching step, the vectorial representation of the input face is compared to the vectorial representation of other faces. If the representations have a high \"similarity\" it is likely that the faces belong to the same person."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<h1>Face recognition pipeline with BlazeFace and ArcFace-MobileNet</h1>\n<p>In this example we want to build a modern face detection pipeline that is capable of identifying the same person in different pictures. A modern face detection pipeline typically consists of these steps:</p>\n<ol>\n<li>Face detection</li>\n<li>Face alignment</li>\n<li>Feature extraction</li>\n<li>Feature matching</li>\n</ol>\n<p>In the face detection step the algorithm tries to find all faces in the given picture. In this example we will use the <a href=\"https://arxiv.org/abs/1907.05047\">BlazeFace</a> neural network to perform the detection task. At the end of the step each face gets cut out into their own image.</p>\n<p>The next steps are performed on each face individually. The faces in the original picture may be tilted or have different sizes depending on their distance from the camera. To improve the performance each face gets straightened and resized to the same size.</p>\n<p>In the feature extraction step the input face is mapped into a vectorial representation called an embedding. In this example we will use a <a href=\"https://arxiv.org/abs/1704.04861\">MobileNet</a> model that has been fine-tuned with the <a href=\"https://arxiv.org/abs/1801.07698\">ArcFace</a> loss function for high accurracy. The model was taken from this <a href=\"https://github.com/leondgarse/Keras_insightface\">Keras ArcFace</a> implementation and converted to the Tensorflowjs format.</p>\n<p>In the feature matching step, the vectorial representation of the input face is compared to the vectorial representation of other faces. If the representations have a high &quot;similarity&quot; it is likely that the faces belong to the same person.</p>\n"}}]},{"cell_type":"markdown","metadata":{},"source":["## Imports\n\nAs a first step we will import the Tensorflowjs library and the official Blazeface Tensorflow model."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<h2>Imports</h2>\n<p>As a first step we will import the Tensorflowjs library and the official Blazeface Tensorflow model.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["import * as tf from \"@tensorflow/tfjs\";\nimport * as blazeface from \"@tensorflow-models/blazeface\";\n\ntf.version.tfjs"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["Next, the weights of the BlazeFace model are initialized."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>Next, the weights of the BlazeFace model are initialized.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let blazefaceModel = await blazeface.load();\n\n\"Done\""],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["## Face detection\n\nThe first step of the face recognition pipeline is to find and extract all faces in the source image. In this example we will focus of just one face. So let's display the image we want to work with."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<h2>Face detection</h2>\n<p>The first step of the face recognition pipeline is to find and extract all faces in the source image. In this example we will focus of just one face. So let's display the image we want to work with.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let img = document.createElement(\"img\");\nimg.height = 426\nimg.width = 640\nimg.src = 'https://images.pexels.com/photos/3757028/pexels-photo-3757028.jpeg'\nimg.crossOrigin = 'anonymous'\nimg"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["Even though the face is clearly visible, the current image is not suited for passing it to the feature extraction model. Firstly, the face occupies only a small part of the image and secondly the face is tilted. So let's use the BlazeFace model to detect the face and get the position of the face."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>Even though the face is clearly visible, the current image is not suited for passing it to the feature extraction model. Firstly, the face occupies only a small part of the image and secondly the face is tilted. So let's use the BlazeFace model to detect the face and get the position of the face.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let imgTensor = tf.browser.fromPixels(img)\n\nlet faces = await blazefaceModel.estimateFaces(imgTensor);\n\nfaces"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["The BlazeFace model has located the face in the image. Now we need to prepare it for the feature extraction model. The feature extraction model requires an image with `112` by `112` pixels. However, since we might need to rotate the picture we will make a larger cutout which we will crop to the right size after rotation. Otherwise the rotation might truncate some edges and leave some black areas behind."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>The BlazeFace model has located the face in the image. Now we need to prepare it for the feature extraction model. The feature extraction model requires an image with <code>112</code> by <code>112</code> pixels. However, since we might need to rotate the picture we will make a larger cutout which we will crop to the right size after rotation. Otherwise the rotation might truncate some edges and leave some black areas behind.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let pixels = 112;\n\nlet cutoutPixels = Math.floor(Math.sqrt(2)*pixels)\n\ncutoutPixels"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["After calculating the required pixels we create the according square cutout."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>After calculating the required pixels we create the according square cutout.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let cutoutWidth = faces[0].bottomRight[0]-faces[0].topLeft[0];\nlet cutoutHeight = faces[0].bottomRight[1]-faces[0].topLeft[1];\n\nlet cutoutSize = cutoutPixels/pixels*Math.max(cutoutWidth, cutoutHeight);\n\nlet cutoutMiddle = [(faces[0].topLeft[0]+faces[0].bottomRight[0])/2.0,(faces[0].topLeft[1]+faces[0].bottomRight[1])/2.0]\n\nlet normCoords = [[(cutoutMiddle[1]-0.5*cutoutSize)/img.height,\n                  (cutoutMiddle[0]-0.5*cutoutSize)/img.width,\n                  (cutoutMiddle[1]+0.5*cutoutSize)/img.height,\n                  (cutoutMiddle[0]+0.5*cutoutSize)/img.width]];\n\nlet cutout = tf.image.cropAndResize(imgTensor.expandDims(0), normCoords, [0], [cutoutPixels, cutoutPixels]).squeeze();\n\nlet normalized = cutout.divNoNan(tf.scalar(255))\n\nnormalized.shape"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["Let's have a look at what BlazeFace found."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>Let's have a look at what BlazeFace found.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let imgData = new ImageData(await tf.browser.toPixels(normalized), cutoutPixels, cutoutPixels);\n\nlet canvas = document.createElement(\"canvas\")\nlet ctx = canvas.getContext(\"2d\");\nctx.putImageData(imgData, 0, 0);\n\nlet image = document.createElement(\"img\")\nimage.src = canvas.toDataURL();\nimage"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["## Face alignment\n\nIn the next step we straighten the image. In addition to the bounding box, the BlazeFace model provides us with \"landmarks\" of the faces that it finds. The landmarks are: left eye, right eye, nose, mouth, left ear, right ear. To straighten the image we will use the positions of the eyes to calculate the angle by which the image has to be rotated for it to be straight. "],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<h2>Face alignment</h2>\n<p>In the next step we straighten the image. In addition to the bounding box, the BlazeFace model provides us with &quot;landmarks&quot; of the faces that it finds. The landmarks are: left eye, right eye, nose, mouth, left ear, right ear. To straighten the image we will use the positions of the eyes to calculate the angle by which the image has to be rotated for it to be straight.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let leftEye = faces[0].landmarks[0];\nlet rightEye = faces[0].landmarks[1];\n\nlet angle = tf.atan(tf.scalar((rightEye[1]-leftEye[1])/(rightEye[0]-leftEye[0]))).arraySync();\n\nlet rotated = tf.image.rotateWithOffset(normalized.expandDims(0), angle).squeeze();\n\nrotated.shape"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["After we have rotated the image we can crop it to the right `112` by `112` pixel size we need."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>After we have rotated the image we can crop it to the right <code>112</code> by <code>112</code> pixel size we need.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let normCoords = [[(cutoutPixels-pixels)/2.0/cutoutPixels,\n                  (cutoutPixels-pixels)/2.0/cutoutPixels,\n                  1.0 - (cutoutPixels-pixels)/2.0/cutoutPixels,\n                  1.0 - (cutoutPixels-pixels)/2.0/cutoutPixels]];\n\nlet faceImg = tf.image.cropAndResize(rotated.expandDims(0), normCoords, [0], [pixels, pixels]).squeeze();\n\nfaceImg.shape"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["Let's display the rotated image. It is not a 100% straigth but we still take it."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>Let's display the rotated image. It is not a 100% straigth but we still take it.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let imgData = new ImageData(await tf.browser.toPixels(faceImg), pixels, pixels);\n\nlet canvas = document.createElement(\"canvas\")\ncanvas.width = 112\ncanvas.height = 112\nlet ctx = canvas.getContext(\"2d\");\nctx.putImageData(imgData, 0, 0);\n\nlet image = document.createElement(\"img\")\nimage.src = canvas.toDataURL();\nimage"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["## Feature extraction\n\nFeature extraction is the task of creating a vectorial representation, called an \"embedding\", of an image. In the image the content is stored by the spatial arrangement of the pixels. An embedding of an image stores the content with more mathematical \"structure\". The embedding can be thought of as a mathematical vector that can be compared to other vectors. How exactly the content is \"encoded\" in the embedding is captured by the weights of the neural network that computes the embedding. The \"encoding\" can typically not be fully understood by humans.\n\nIn this example we will use the [MobileNet](https://arxiv.org/abs/1704.04861) neural network that has been fine-tuned for face recognition tasks with the [ArcFace](https://arxiv.org/abs/1801.07698) loss function to compute the embedding for our image. The model was taken from this [Keras ArcFace](https://github.com/leondgarse/Keras_insightface) implementation and converted to the Tensorflowjs format.\n\nIn the next cell we load the pretrained model."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<h2>Feature extraction</h2>\n<p>Feature extraction is the task of creating a vectorial representation, called an &quot;embedding&quot;, of an image. In the image the content is stored by the spatial arrangement of the pixels. An embedding of an image stores the content with more mathematical &quot;structure&quot;. The embedding can be thought of as a mathematical vector that can be compared to other vectors. How exactly the content is &quot;encoded&quot; in the embedding is captured by the weights of the neural network that computes the embedding. The &quot;encoding&quot; can typically not be fully understood by humans.</p>\n<p>In this example we will use the <a href=\"https://arxiv.org/abs/1704.04861\">MobileNet</a> neural network that has been fine-tuned for face recognition tasks with the <a href=\"https://arxiv.org/abs/1801.07698\">ArcFace</a> loss function to compute the embedding for our image. The model was taken from this <a href=\"https://github.com/leondgarse/Keras_insightface\">Keras ArcFace</a> implementation and converted to the Tensorflowjs format.</p>\n<p>In the next cell we load the pretrained model.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let mobilenet = await tf.loadLayersModel(\"https://storage.googleapis.com/weblab_models/arcface-mobilenet/model.json\");\n\n\"Done\""],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["Let's use the model to compute the embedding for our face."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>Let's use the model to compute the embedding for our face.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let features = mobilenet.predict(faceImg.expandDims(0)).squeeze()\n\nfeatures.toString()"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["As mentioned before, it is difficult to interpret the embedding. However we can develop an intuition when we compare the embedding against embeddings of other images. This is what we will do in the next step."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>As mentioned before, it is difficult to interpret the embedding. However we can develop an intuition when we compare the embedding against embeddings of other images. This is what we will do in the next step.</p>\n"}}]},{"cell_type":"markdown","metadata":{},"source":["## Feature matching\n\nNow that we have computed the embedding representation for our image, we can see how it compares to other embeddings. The idea is that different images of the same face form clusters once we plot them into the embedding space. This shall be illustrated in the following image. We can see that images of the same person appear \"close\" to each other.\n\n![clustering](https://imgur.com/dPaURX8.png)\n\nIf we now want to find the person inside of an image, we compare its embedding against the embeddings of images we have already collected. If the embedding is \"similar\" to an embedding of an existing person we can classify them as the same person.\n\nLet's try it out for our example. Since we haven't collected any embeddings beforehand we will compute some embeddings for other images. Let's first look at another image of the same person."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<h2>Feature matching</h2>\n<p>Now that we have computed the embedding representation for our image, we can see how it compares to other embeddings. The idea is that different images of the same face form clusters once we plot them into the embedding space. This shall be illustrated in the following image. We can see that images of the same person appear &quot;close&quot; to each other.</p>\n<p><img src=\"https://imgur.com/dPaURX8.png\" alt=\"clustering\" /></p>\n<p>If we now want to find the person inside of an image, we compare its embedding against the embeddings of images we have already collected. If the embedding is &quot;similar&quot; to an embedding of an existing person we can classify them as the same person.</p>\n<p>Let's try it out for our example. Since we haven't collected any embeddings beforehand we will compute some embeddings for other images. Let's first look at another image of the same person.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let sameImg = document.createElement(\"img\");\nsameImg.height = 426\nsameImg.width = 640\nsameImg.src = 'https://images.pexels.com/photos/3760743/pexels-photo-3760743.jpeg'\nsameImg.crossOrigin = 'anonymous'\nsameImg"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["In the next cell we will do all the steps that we did for the first image and compute the embedding at the end."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>In the next cell we will do all the steps that we did for the first image and compute the embedding at the end.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let imgTensor = tf.browser.fromPixels(sameImg)\n\nlet faces = await blazefaceModel.estimateFaces(imgTensor);\n\nlet cutoutWidth = faces[0].bottomRight[0]-faces[0].topLeft[0];\nlet cutoutHeight = faces[0].bottomRight[1]-faces[0].topLeft[1];\n\nlet cutoutSize = cutoutPixels/pixels*Math.max(cutoutWidth,cutoutHeight);\n\nlet cutoutMiddle = [(faces[0].topLeft[0]+faces[0].bottomRight[0])/2.0,(faces[0].topLeft[1]+faces[0].bottomRight[1])/2.0]\n\nlet normCoords = [[(cutoutMiddle[1]-0.5*cutoutSize)/sameImg.height,\n                  (cutoutMiddle[0]-0.5*cutoutSize)/sameImg.width,\n                  (cutoutMiddle[1]+0.5*cutoutSize)/sameImg.height,\n                  (cutoutMiddle[0]+0.5*cutoutSize)/sameImg.width]];\n\nlet cutout = tf.image.cropAndResize(imgTensor.expandDims(0), normCoords, [0], [cutoutPixels, cutoutPixels]).squeeze();\n\nlet normalized = cutout.divNoNan(tf.scalar(255))\n\nlet leftEye = faces[0].landmarks[0];\nlet rightEye = faces[0].landmarks[1];\n\nlet angle = tf.atan(tf.scalar((rightEye[1]-leftEye[1])/(rightEye[0]-leftEye[0]))).arraySync();\n\nlet rotated = tf.image.rotateWithOffset(normalized.expandDims(0), angle).squeeze()\n\nlet normCoords = [[(cutoutPixels-pixels)/2.0/cutoutPixels,\n                  (cutoutPixels-pixels)/2.0/cutoutPixels,\n                  1.0 - (cutoutPixels-pixels)/2.0/cutoutPixels,\n                  1.0 - (cutoutPixels-pixels)/2.0/cutoutPixels]];\n\nlet faceImg = tf.image.cropAndResize(rotated.expandDims(0), normCoords, [0], [pixels, pixels]).squeeze();\n\nlet sameFeatures = mobilenet.predict(faceImg.expandDims(0)).squeeze()\n\nsameFeatures.toString()"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["Let's have a look at how the image looks like that we pass to the feature extractor."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>Let's have a look at how the image looks like that we pass to the feature extractor.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let imgData = new ImageData(await tf.browser.toPixels(faceImg), pixels, pixels);\n\nlet canvas = document.createElement(\"canvas\")\nlet ctx = canvas.getContext(\"2d\");\nctx.putImageData(imgData, 0, 0);\n\nlet image = document.createElement(\"img\")\nimage.src = canvas.toDataURL();\nimage"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["Now we can compare the embeddings of both images. There a different metrics that can be used to assess the similarity of the embeddings. The most common are the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) and the [euclidean norm](https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm). Both have their strengths and weaknesses. Let us use the cosine similarity to compare both embeddings."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>Now we can compare the embeddings of both images. There a different metrics that can be used to assess the similarity of the embeddings. The most common are the <a href=\"https://en.wikipedia.org/wiki/Cosine_similarity\">cosine similarity</a> and the <a href=\"https://en.wikipedia.org/wiki/Norm_(mathematics)#Euclidean_norm\">euclidean norm</a>. Both have their strengths and weaknesses. Let us use the cosine similarity to compare both embeddings.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["tf.dot(features, sameFeatures).div(tf.norm(features).mul(tf.norm(sameFeatures))).arraySync()"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["The value on its own is not very meaningful, it has to be compared to other values. Let's have a look at another image, to see the difference."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>The value on its own is not very meaningful, it has to be compared to other values. Let's have a look at another image, to see the difference.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let otherImg = document.createElement(\"img\");\notherImg.height = 426\notherImg.width = 640\notherImg.src = 'https://images.pexels.com/photos/3762781/pexels-photo-3762781.jpeg'\notherImg.crossOrigin = 'anonymous'\notherImg"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["Again, we will compute the embedding as before."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>Again, we will compute the embedding as before.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let imgTensor = tf.browser.fromPixels(otherImg)\n\nlet faces = await blazefaceModel.estimateFaces(imgTensor);\n\nlet cutoutWidth = faces[0].bottomRight[0]-faces[0].topLeft[0];\nlet cutoutHeight = faces[0].bottomRight[1]-faces[0].topLeft[1];\n\nlet cutoutSize = cutoutPixels/pixels*Math.max(cutoutWidth,cutoutHeight);\n\nlet cutoutMiddle = [(faces[0].topLeft[0]+faces[0].bottomRight[0])/2.0,(faces[0].topLeft[1]+faces[0].bottomRight[1])/2.0]\n\nlet normCoords = [[(cutoutMiddle[1]-0.5*cutoutSize)/otherImg.height,\n                  (cutoutMiddle[0]-0.5*cutoutSize)/otherImg.width,\n                  (cutoutMiddle[1]+0.5*cutoutSize)/otherImg.height,\n                  (cutoutMiddle[0]+0.5*cutoutSize)/otherImg.width]];\n\nlet cutout = tf.image.cropAndResize(imgTensor.expandDims(0), normCoords, [0], [cutoutPixels, cutoutPixels]).squeeze();\n\nlet normalized = cutout.divNoNan(tf.scalar(255))\n\nlet leftEye = faces[0].landmarks[0];\nlet rightEye = faces[0].landmarks[1];\n\nlet angle = tf.atan(tf.scalar((rightEye[1]-leftEye[1])/(rightEye[0]-leftEye[0]))).arraySync();\n\nlet rotated = tf.image.rotateWithOffset(normalized.expandDims(0), angle).squeeze()\n\nlet normCoords = [[(cutoutPixels-pixels)/2.0/cutoutPixels,\n                  (cutoutPixels-pixels)/2.0/cutoutPixels,\n                  1.0 - (cutoutPixels-pixels)/2.0/cutoutPixels,\n                  1.0 - (cutoutPixels-pixels)/2.0/cutoutPixels]];\n\nlet faceImg = tf.image.cropAndResize(rotated.expandDims(0), normCoords, [0], [pixels, pixels]).squeeze();\n\nlet otherFeatures = mobilenet.predict(faceImg.expandDims(0)).squeeze()\n\notherFeatures.toString()"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["The cropped and rotated image looks as follows."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>The cropped and rotated image looks as follows.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let imgData = new ImageData(await tf.browser.toPixels(faceImg), pixels, pixels);\n\nlet canvas = document.createElement(\"canvas\")\nlet ctx = canvas.getContext(\"2d\");\nctx.putImageData(imgData, 0, 0);\n\nlet image = document.createElement(\"img\")\nimage.src = canvas.toDataURL();\nimage"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["And finally let's compute the cosine similarity of the last image and our initial image."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>And finally let's compute the cosine similarity of the last image and our initial image.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["tf.dot(features, otherFeatures).div(tf.norm(features).mul(tf.norm(otherFeatures))).arraySync()"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["The score is much lower, which indicates different persons in the images. Consequently we would classify the person in our initial image to be the same person as the one in the first image we compared it against.\n\nIt is not easy to say which values can be considered as the same person. [Here](https://sefiks.com/2020/05/22/fine-tuning-the-threshold-in-face-recognition/) is a guide that helps to adjust the threshold value for face recognition.\n\nTypically the embeddings of existing users are stored in a database in the back-end and you will need a special algorithm to find a suitable embedding."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>The score is much lower, which indicates different persons in the images. Consequently we would classify the person in our initial image to be the same person as the one in the first image we compared it against.</p>\n<p>It is not easy to say which values can be considered as the same person. <a href=\"https://sefiks.com/2020/05/22/fine-tuning-the-threshold-in-face-recognition/\">Here</a> is a guide that helps to adjust the threshold value for face recognition.</p>\n<p>Typically the embeddings of existing users are stored in a database in the back-end and you will need a special algorithm to find a suitable embedding.</p>\n"}}]}]}
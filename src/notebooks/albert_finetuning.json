{"nbformat":4,"nbformat_minor":0,"metadata":{"kernel_info":{"name":"Weblab"},"language_info":{"name":"javascript"}},"cells":[{"cell_type":"markdown","metadata":{},"source":["# Fine-tuning an ALBERT encoder for classification tasks\n\nIn this example we want to train an intent classifier. \nThe goal of the intent classifier is to take queries in the form of text and output the intent behind the query. \nWe will do this in the context of queries related to bank accounts. \nThe training dataset is taken from [PolyAI-LDN](https://github.com/PolyAI-LDN/task-specific-datasets) and consists of queries like \"Is there a way to know when my card will arrive?\" and their related intents like \"card_arrival\". \nIn total the dataset contains queries related to 77 different intents.\n\n\nWe will perform the classification task with a deep neural network using the [ALBERT encoder](https://github.com/google-research/albert).\nSince this will lead to a very large deep neural network, that requires a lot of computational effort, we will use **transfer learning** to speed up the training process.\nLuckily, the ALBERT repository provides a model that has been pretrained on the Wikipedia text corpus.\nThe tokenization of the input text is done with the [sentencepiece text tokenizer](https://github.com/google/sentencepiece).\n\n\nPlease keep in mind that the training of the whole ALBERT model requires performant hardware resources and can probably not be performed on a laptop. For the case of limited hardware resourced we provide an alternative in which only parts of the model will be trained."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<h1>Fine-tuning an ALBERT encoder for classification tasks</h1>\n<p>In this example we want to train an intent classifier.\nThe goal of the intent classifier is to take queries in the form of text and output the intent behind the query.\nWe will do this in the context of queries related to bank accounts.\nThe training dataset is taken from <a href=\"https://github.com/PolyAI-LDN/task-specific-datasets\">PolyAI-LDN</a> and consists of queries like &quot;Is there a way to know when my card will arrive?&quot; and their related intents like &quot;card_arrival&quot;.\nIn total the dataset contains queries related to 77 different intents.</p>\n<p>We will perform the classification task with a deep neural network using the <a href=\"https://github.com/google-research/albert\">ALBERT encoder</a>.\nSince this will lead to a very large deep neural network, that requires a lot of computational effort, we will use <strong>transfer learning</strong> to speed up the training process.\nLuckily, the ALBERT repository provides a model that has been pretrained on the Wikipedia text corpus.\nThe tokenization of the input text is done with the <a href=\"https://github.com/google/sentencepiece\">sentencepiece text tokenizer</a>.</p>\n<p>Please keep in mind that the training of the whole ALBERT model requires performant hardware resources and can probably not be performed on a laptop. For the case of limited hardware resourced we provide an alternative in which only parts of the model will be trained.</p>\n"}}]},{"cell_type":"markdown","metadata":{},"source":["## Import required libraries\n\nTo perform the preprocessing and the training we will need the following libraries.\nWe will use the sentencepiece library for the text tokenization. The training will be done with the **Tensorflow.js** library. Since the ALBERT encoder isn't available as an official model yet, we also need to import the `Albert` model class. Additionally we need the library papaparse to parse .csv files into the JSON format."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<h2>Import required libraries</h2>\n<p>To perform the preprocessing and the training we will need the following libraries.\nWe will use the sentencepiece library for the text tokenization. The training will be done with the <strong>Tensorflow.js</strong> library. Since the ALBERT encoder isn't available as an official model yet, we also need to import the <code>Albert</code> model class. Additionally we need the library papaparse to parse .csv files into the JSON format.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["import {cleanText, sentencePieceProcessor} from \"@weblab-notebook/sentencepiece\";\nimport {tf, Albert} from \"@weblab-notebook/albert_encoder\";\nimport * as papaparse from 'papaparse';\n\ntf.version.tfjs"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["## Fetch the training data\n\nAs a next step we need to get the training and validation data. Both are available as .csv files. With the following commands we fetch the .csv files and save their text."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<h2>Fetch the training data</h2>\n<p>As a next step we need to get the training and validation data. Both are available as .csv files. With the following commands we fetch the .csv files and save their text.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let trainFile = await fetch(\"https://raw.githubusercontent.com/PolyAI-LDN/task-specific-datasets/master/banking_data/train.csv\").then(response => response.text());\n\nlet validationFile = await fetch(\"https://raw.githubusercontent.com/PolyAI-LDN/task-specific-datasets/master/banking_data/test.csv\").then(response => response.text());\n\n\"Done fetching\""],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["To make it easier to handle the data we use the papaparse library to convert the text in the csv format into a JSON object. Once we have done that, we can look at the entries individually. Each entry combines a query text and the corresponding intent."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>To make it easier to handle the data we use the papaparse library to convert the text in the csv format into a JSON object. Once we have done that, we can look at the entries individually. Each entry combines a query text and the corresponding intent.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let trainData = papaparse.parse(trainFile, {\n\theader: true\n}).data;\ntrainData.pop()\n\nlet validationData = papaparse.parse(validationFile, {\n\theader: true\n}).data;\nvalidationData.pop()\n\ntrainData[0]"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["In total there are 77 different intents. We can get a list of them from the given repository."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>In total there are 77 different intents. We can get a list of them from the given repository.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let intentsFile = await fetch(\"https://raw.githubusercontent.com/PolyAI-LDN/task-specific-datasets/master/banking_data/categories.json\").then(response => response.json());\n\nintentsFile[2]"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["Now that we've collected all the data, it is time to put it in a form that the ALBERT encoder understands. We have to transform the queries as well as the intents into a numerical form. Lets start with the intents.\n\nThe task that we actually want to perform is called multiclass classification, where each query can only be labeled with one intent.\nFor multiclass classification tasks it is common to represent the model output as a vector with the length of the number of intents. Each intent is then characterized by a vector with a \"1\" at its corresponding position and \"0\" everywhere else. This is called a one-hot encoded vector.\n\nFor example:\n```\n[ 1, 0, 0, 0, 0, 0, 0, ...] intent: \"card_arrival\"\n```\n```\n[ 0, 1, 0, 0, 0, 0, 0, ...] intent: \"card_linking\"\n```\n```\n[ 0, 0, 1, 0, 0, 0, 0, ...] intent: \"exchange_rate\"\n```\n\nIn the next cell we create an object that maps each intent to its corresponding one-hot encoded vector."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>Now that we've collected all the data, it is time to put it in a form that the ALBERT encoder understands. We have to transform the queries as well as the intents into a numerical form. Lets start with the intents.</p>\n<p>The task that we actually want to perform is called multiclass classification, where each query can only be labeled with one intent.\nFor multiclass classification tasks it is common to represent the model output as a vector with the length of the number of intents. Each intent is then characterized by a vector with a &quot;1&quot; at its corresponding position and &quot;0&quot; everywhere else. This is called a one-hot encoded vector.</p>\n<p>For example:</p>\n<pre><code>[ 1, 0, 0, 0, 0, 0, 0, ...] intent: &quot;card_arrival&quot;\n</code></pre>\n<pre><code>[ 0, 1, 0, 0, 0, 0, 0, ...] intent: &quot;card_linking&quot;\n</code></pre>\n<pre><code>[ 0, 0, 1, 0, 0, 0, 0, ...] intent: &quot;exchange_rate&quot;\n</code></pre>\n<p>In the next cell we create an object that maps each intent to its corresponding one-hot encoded vector.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let numClasses = intentsFile.length;\n\nlet intents = intentsFile.reduce((acc,x,i) => {\n  acc[x] = tf.oneHot(i, numClasses);\n  return acc;\n}, {});\n\nintents[\"exchange_rate\"].toString()"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["With that we can convert every intent to its one-hot encoded representation."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>With that we can convert every intent to its one-hot encoded representation.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["trainData = trainData.map(x => {return {text: x.text, intent: intents[x.category]}});\n\nvalidationData = validationData.map(x => {return {text: x.text, intent: intents[x.category]}});\n\n\"Done\""],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["## Tokenize the training sentences\n\nNow that we have intents ready, we have to get the queries into a numerical representation. For that the input is split into tokens. These can be individual words like \"the, cat\" or part of words like \"ing, ed\". These tokens are then translated into numbers using a vocabulary where every token is assigned a unique number.\n\nWe can convert the input text into token arrays using the sentencepiece library. The following command creates a sentencepiece preprocessor object from a given model file."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<h2>Tokenize the training sentences</h2>\n<p>Now that we have intents ready, we have to get the queries into a numerical representation. For that the input is split into tokens. These can be individual words like &quot;the, cat&quot; or part of words like &quot;ing, ed&quot;. These tokens are then translated into numbers using a vocabulary where every token is assigned a unique number.</p>\n<p>We can convert the input text into token arrays using the sentencepiece library. The following command creates a sentencepiece preprocessor object from a given model file.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let preprocessor = await sentencePieceProcessor(\"https://storage.googleapis.com/weblab_models/albert_base/30k-clean.model\");"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["Let's try the preprocessor in action. We will first run the function `cleanText()` to clean up the text und convert it to lowercase. Converting the text \"Hello to tensorflow!\" yields the following indices:"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>Let's try the preprocessor in action. We will first run the function <code>cleanText()</code> to clean up the text und convert it to lowercase. Converting the text &quot;Hello to tensorflow!&quot; yields the following indices:</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let ids = preprocessor.encodeIds(cleanText(\"Hello to tensorflow!\"));\n\nids.toString()"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["To see wether it worked we can decode the ids and check the resulting text. Keep in mind that we converted the text to lowercase beforehand."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>To see wether it worked we can decode the ids and check the resulting text. Keep in mind that we converted the text to lowercase beforehand.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let text = preprocessor.decodeIds(ids);\ntext"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["We are now ready to convert our input sentences to their corresponding ids."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>We are now ready to convert our input sentences to their corresponding ids.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["trainData = trainData.map((x) => {\n  let cleaned = cleanText(x.text);\n  let tokenIds = preprocessor.encodeIds(cleaned);\n  return {ids: tokenIds, intent: x.intent}\n});\n\nvalidationData = validationData.map((x) => {\n  let cleaned = cleanText(x.text);\n  let tokenIds = preprocessor.encodeIds(cleaned);\n  return {ids: tokenIds, intent: x.intent}\n});\n\ntrainData[0].ids.toString()"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["In addition to the input ids the encoder requires certain \"control tokens\" that convey semantic meaning to the model. The textual representation of the control tokens are `[CLS]`, `[SEP]` and `[PAD]`. Each control token serves a different function in the input sequence. The `[CLS]` token has to appear at the beginning of a sentence and specifies that we want to perform a classification task. The `[CLS]` token is then followed by one of the input sentences. The end of the input sentence is marked by a `[SEP]` token. In case of a sentence pair classification, another - sentence `[SEP]` - pair has to be appended. To achieve a fixed length input, `[PAD]` tokens are added to the end until a predetermined maximum sequence length is obtained.\n\nLet's look at an example. The first input sentence has to look like this:\n\n```\n[CLS] I am still waiting on my card? [SEP] [PAD] [PAD] [PAD] ...\n```\n\nThe indices of the control tokens are as follows:\n\n- `[CLS]`: 2\n- `[SEP]`: 3\n- `[PAD]`: 0\n\nTo get the right format we need to prepend the `[CLS]` token \"2\" to the input sentence, then append the `[SEP]` token \"3\" and fill the rest of our input with `[PAD]` tokens \"0\". Let's look at the example from before.\n\nFormatted input sentence:\n```\n[CLS] I am still waiting on my card? [SEP] [PAD] [PAD] [PAD] ...\n```\nCorresponding input indices:\n```\n[   2,   31,589,174,1672,27,51,2056,60,   3,   0,   0,   0,   0 ... ]\n```\nWith the indices of our input sentences in the right format we need two additional inputs for the ALBERT model, the `typeIds` and the `intputMask`. The `typeIds` are needed for sentence pair classification. The `typeIds` tensor has the same length as the `inputIds` and has the value \"0\" at every index of the first sentence and the value \"1\" at every index of the second sentence. In this example, with only one sentence, it contains only \"0\"s.\n\nThe `inputMask` marks the non-padded region. It contains the value \"1\" where there is no padding and \"0\" where there is padding.\n\nIn the next cell we prepare the input for the training process. Therefore we perform all the steps that we just talked about. For this example we restrict the sequence length to *32* to reduce the computational effort required for training. This truncates some sentences and is typically not what you want to do. If you have the appropriate hardware available feel free to set the `sequenceLength` to *128*. "],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>In addition to the input ids the encoder requires certain &quot;control tokens&quot; that convey semantic meaning to the model. The textual representation of the control tokens are <code>[CLS]</code>, <code>[SEP]</code> and <code>[PAD]</code>. Each control token serves a different function in the input sequence. The <code>[CLS]</code> token has to appear at the beginning of a sentence and specifies that we want to perform a classification task. The <code>[CLS]</code> token is then followed by one of the input sentences. The end of the input sentence is marked by a <code>[SEP]</code> token. In case of a sentence pair classification, another - sentence <code>[SEP]</code> - pair has to be appended. To achieve a fixed length input, <code>[PAD]</code> tokens are added to the end until a predetermined maximum sequence length is obtained.</p>\n<p>Let's look at an example. The first input sentence has to look like this:</p>\n<pre><code>[CLS] I am still waiting on my card? [SEP] [PAD] [PAD] [PAD] ...\n</code></pre>\n<p>The indices of the control tokens are as follows:</p>\n<ul>\n<li><code>[CLS]</code>: 2</li>\n<li><code>[SEP]</code>: 3</li>\n<li><code>[PAD]</code>: 0</li>\n</ul>\n<p>To get the right format we need to prepend the <code>[CLS]</code> token &quot;2&quot; to the input sentence, then append the <code>[SEP]</code> token &quot;3&quot; and fill the rest of our input with <code>[PAD]</code> tokens &quot;0&quot;. Let's look at the example from before.</p>\n<p>Formatted input sentence:</p>\n<pre><code>[CLS] I am still waiting on my card? [SEP] [PAD] [PAD] [PAD] ...\n</code></pre>\n<p>Corresponding input indices:</p>\n<pre><code>[   2,   31,589,174,1672,27,51,2056,60,   3,   0,   0,   0,   0 ... ]\n</code></pre>\n<p>With the indices of our input sentences in the right format we need two additional inputs for the ALBERT model, the <code>typeIds</code> and the <code>intputMask</code>. The <code>typeIds</code> are needed for sentence pair classification. The <code>typeIds</code> tensor has the same length as the <code>inputIds</code> and has the value &quot;0&quot; at every index of the first sentence and the value &quot;1&quot; at every index of the second sentence. In this example, with only one sentence, it contains only &quot;0&quot;s.</p>\n<p>The <code>inputMask</code> marks the non-padded region. It contains the value &quot;1&quot; where there is no padding and &quot;0&quot; where there is padding.</p>\n<p>In the next cell we prepare the input for the training process. Therefore we perform all the steps that we just talked about. For this example we restrict the sequence length to <em>32</em> to reduce the computational effort required for training. This truncates some sentences and is typically not what you want to do. If you have the appropriate hardware available feel free to set the <code>sequenceLength</code> to <em>128</em>.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let sequenceLength = 32;\n\ntrainData = trainData.filter(x => {\n  return x.ids.length <= sequenceLength - 2}\n).map(x => {\n  return tf.tidy(() => {\n    \n    let inputLength = x.ids.length + 2;\n    \n    let inputIds = tf.concat([tf.tensor1d([2]), tf.tensor1d(x.ids.slice(0, sequenceLength - 2)), tf.tensor1d([3]), tf.zeros([sequenceLength-inputLength])]);\n    let typeIds = tf.zeros([sequenceLength]);\n    let inputMask = tf.concat([tf.ones([inputLength]), tf.zeros([sequenceLength-inputLength])]);\n    \n    return {xs: {input1: inputIds, input2: typeIds, input3: inputMask}, ys: x.intent}\n  })\n});\n\ntrainData[0].xs.input1.slice(0,16).toString()"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["We will do the same with the validation data."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>We will do the same with the validation data.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["validationData = validationData.filter(x => {\n  return x.ids.length <= sequenceLength - 2}\n).map(x => {\n  return tf.tidy(() => {\n    let inputLength = x.ids.length + 2;\n    \n    let inputIds = tf.concat([tf.tensor1d([2]), tf.tensor1d(x.ids.slice(0, sequenceLength - 2)), tf.tensor1d([3]), tf.zeros([sequenceLength-inputLength])]);\n    let typeIds = tf.zeros([sequenceLength]);\n    let inputMask = tf.concat([tf.ones([inputLength]), tf.zeros([sequenceLength-inputLength])]);\n    \n    return {xs: {input1: inputIds, input2: typeIds, input3: inputMask}, ys: x.intent}\n  })\n});\n\n\"Done\""],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["## Build the ALBERT classifier\n\nWith the training data in the correct shape, we can start building the deep neural network that will perform the classification task. The general architecture of our neural network is as follows:\n\n1. Input layer\n2. Albert layer\n    1. Embedding\n    2. Encoder\n    3. Pooler\n3. Classification layer\n\nThe first layer is used as the input layer. It takes the three inputs `inputIds, segmentIds, attentionMask`. Each one is of type integer."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<h2>Build the ALBERT classifier</h2>\n<p>With the training data in the correct shape, we can start building the deep neural network that will perform the classification task. The general architecture of our neural network is as follows:</p>\n<ol>\n<li>Input layer</li>\n<li>Albert layer\n<ol>\n<li>Embedding</li>\n<li>Encoder</li>\n<li>Pooler</li>\n</ol>\n</li>\n<li>Classification layer</li>\n</ol>\n<p>The first layer is used as the input layer. It takes the three inputs <code>inputIds, segmentIds, attentionMask</code>. Each one is of type integer.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let idsInput = tf.input({ shape: [sequenceLength,], dtype: \"int32\" });\nlet segmentIdsInput = tf.input({ shape: [sequenceLength,], dtype: \"int32\" });\nlet attentionMaskInput = tf.input({ shape: [sequenceLength,], dtype: \"int32\" });"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["The next layer is the ALBERT layer, which itself consists of the three internal layers, the embedding, the encoder, and the pooler layer. The embedding layer turns every token id into a vector representation of that token. The concept of representing text tokens as vectors can initially be hard to grasp. You can find out more about word embeddings in the [tensorflow documentation](https://www.tensorflow.org/text/guide/word_embeddings?hl=en). The goal is to have a mathematical representation in which \"similar\" words have similar vector representations. So for instance the tokens \"cat\" and \"dog\" are likely more similar than \"cat\" and \"rocket\". The benefit of the vector representation is that you can \"measure\" the similarity by performing the scalar product or the cosine similarity.\n\nThe second internal layer of the ALBERT layer is the encoder itself. It is a rather complex design that uses the transformer architecture. The ALBERT encoder is very similar to a BERT encoder layer, on which you can find more information [here](https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270).\n\nThe last internal layer of the ALBERT layer is the pooler layer. It consists of a dense layer that performs a part of the classification.\n\nFor this example we will load the model architecture and the layer weights from the [pretrained model provided by google](https://github.com/google-research/albert). We can load the base model will the following command:"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>The next layer is the ALBERT layer, which itself consists of the three internal layers, the embedding, the encoder, and the pooler layer. The embedding layer turns every token id into a vector representation of that token. The concept of representing text tokens as vectors can initially be hard to grasp. You can find out more about word embeddings in the <a href=\"https://www.tensorflow.org/text/guide/word_embeddings?hl=en\">tensorflow documentation</a>. The goal is to have a mathematical representation in which &quot;similar&quot; words have similar vector representations. So for instance the tokens &quot;cat&quot; and &quot;dog&quot; are likely more similar than &quot;cat&quot; and &quot;rocket&quot;. The benefit of the vector representation is that you can &quot;measure&quot; the similarity by performing the scalar product or the cosine similarity.</p>\n<p>The second internal layer of the ALBERT layer is the encoder itself. It is a rather complex design that uses the transformer architecture. The ALBERT encoder is very similar to a BERT encoder layer, on which you can find more information <a href=\"https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270\">here</a>.</p>\n<p>The last internal layer of the ALBERT layer is the pooler layer. It consists of a dense layer that performs a part of the classification.</p>\n<p>For this example we will load the model architecture and the layer weights from the <a href=\"https://github.com/google-research/albert\">pretrained model provided by google</a>. We can load the base model will the following command:</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let baseModel = await tf.loadLayersModel(\"https://storage.googleapis.com/weblab_models/albert_base/model.json\");"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["Since we are only interested in the albert layer of the base model, we create a variable that references that layer."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>Since we are only interested in the albert layer of the base model, we create a variable that references that layer.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let albert = baseModel.getLayer(\"albert\");"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["In order to reduce the required computational effort of training the model, we set the weights of the internal embedding layer of the albert layer to not be trainable. This reduces the model size by almost 4 mio. weights and should not lead to a significant accuracy loss."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>In order to reduce the required computational effort of training the model, we set the weights of the internal embedding layer of the albert layer to not be trainable. This reduces the model size by almost 4 mio. weights and should not lead to a significant accuracy loss.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["albert.embedding.trainable = false"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["If the model is still too large, the encoder layer can also be set non-trainable. This will depend on your hardware resources. However the accuracy impact might be larger."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>If the model is still too large, the encoder layer can also be set non-trainable. This will depend on your hardware resources. However the accuracy impact might be larger.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["//albert.encoder.trainable = false"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["Finally, we connect the input layers to our ALBERT layer. The first output of the ALBERT layer are the attention weights, which we don't need for our classification task."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>Finally, we connect the input layers to our ALBERT layer. The first output of the ALBERT layer are the attention weights, which we don't need for our classification task.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let [_, pooledOutput] = albert.apply([idsInput, segmentIdsInput, attentionMaskInput]);"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["After the ALBERT layer we will add another classification layer that transforms the output into the correct form for our specific classification task. For that we use a dense layer with 77 units and a softmax activation function. The softmax functions assigns each class a value between 0 and 1 and additionally all values add up to 1.\n\nTo reduce overfitting we add a dropout layer before the dense layer."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>After the ALBERT layer we will add another classification layer that transforms the output into the correct form for our specific classification task. For that we use a dense layer with 77 units and a softmax activation function. The softmax functions assigns each class a value between 0 and 1 and additionally all values add up to 1.</p>\n<p>To reduce overfitting we add a dropout layer before the dense layer.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let dropout = tf.layers.dropout({rate: 0.1});\nlet dense = tf.layers.dense({units: numClasses, activation: \"softmax\"});"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["Again, we need to connect the dropout layer and the dense layer to the previous ALBERT layer."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>Again, we need to connect the dropout layer and the dense layer to the previous ALBERT layer.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let dropoutOutput = dropout.apply(pooledOutput);\nlet output = dense.apply(dropoutOutput);"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["The model is then created by providing the inputs and outputs of the model."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>The model is then created by providing the inputs and outputs of the model.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let model = tf.model({inputs: [idsInput, segmentIdsInput, attentionMaskInput], outputs: output});"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["You can have a look at the classification model in your browser console with the following command:"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>You can have a look at the classification model in your browser console with the following command:</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["model.summary()"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["## Train the model\n\nWith the model architecture ready, it is time to train the model. The training adjusts the weights of the network such that it minimizes a given loss function. For that we need to specify which loss function we want to minimize. Since we are performing a multiclass classification task we want to use the [categorical Crossentropy loss function](https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/categorical-crossentropy).\n\nThe adjustment to the weights is done by some kind of gradient descent algorithm. The performance of which can greatly be improved by using an optimizer. In this case we will use the [Adam optimizer](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) with a learning rate of *0.00001*. This is a rather small learning rate but since our model consists mostly of weights from transfer learning, we do not want them to change too quickly. They should already have near optimal values.\n\nWe specify the optimizer and loss function for our model with the following command:"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<h2>Train the model</h2>\n<p>With the model architecture ready, it is time to train the model. The training adjusts the weights of the network such that it minimizes a given loss function. For that we need to specify which loss function we want to minimize. Since we are performing a multiclass classification task we want to use the <a href=\"https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/categorical-crossentropy\">categorical Crossentropy loss function</a>.</p>\n<p>The adjustment to the weights is done by some kind of gradient descent algorithm. The performance of which can greatly be improved by using an optimizer. In this case we will use the <a href=\"https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/\">Adam optimizer</a> with a learning rate of <em>0.00001</em>. This is a rather small learning rate but since our model consists mostly of weights from transfer learning, we do not want them to change too quickly. They should already have near optimal values.</p>\n<p>We specify the optimizer and loss function for our model with the following command:</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["model.compile({\n  optimizer: tf.train.adam(0.00001),\n  loss: 'categoricalCrossentropy',\n  metrics: ['accuracy']\n});"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["Next, we create datasets for our training and validation data. With it we can specify the batch size for the training process. The batch sizes typically range from *4* to *64*. Larger batch sizes lead to faster training but also require more powerful hardware. For this example you might need to reduce the batch size to *4*, depending on your hardware."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>Next, we create datasets for our training and validation data. With it we can specify the batch size for the training process. The batch sizes typically range from <em>4</em> to <em>64</em>. Larger batch sizes lead to faster training but also require more powerful hardware. For this example you might need to reduce the batch size to <em>4</em>, depending on your hardware.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let trainDataset = tf.data.array(trainData).shuffle(trainData.length).batch(16);"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let validationDataset = tf.data.array(validationData).shuffle(validationData.length).batch(8);"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["The following cell starts the training process. It will save the model to the browsers IndexedDB every 10 minutes. You can follow the loss function in your browser console."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>The following cell starts the training process. It will save the model to the browsers IndexedDB every 10 minutes. You can follow the loss function in your browser console.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let interval = setInterval(async () => {await model.save('indexeddb://albert_bank')}, 600000)\n\nawait model.fitDataset(trainDataset, {epochs: 1, \n                          validationData: validationDataset, \n                          callbacks: [{onBatchEnd: (batch, logs) => {return console.log(batch, logs.loss)}}]\n                          }).then(_ => {clearInterval(interval)});\n\nawait model.save('indexeddb://albert_bank')"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["In case you want to skip the training process, you can load an already trained model from the following url:"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>In case you want to skip the training process, you can load an already trained model from the following url:</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["model = await tf.loadLayersModel('https://storage.googleapis.com/weblab_models/albert_bank/model.json');\n\n\"Done\""],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["## Make predictions\n\nTo see whether the training was successful we can now look at some predictions. The following cell converts an input sentence into the required input tensors for the classification model."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<h2>Make predictions</h2>\n<p>To see whether the training was successful we can now look at some predictions. The following cell converts an input sentence into the required input tensors for the classification model.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let input = \"My money withdrawal is not working.\"\n\nlet testIds = preprocessor.encodeIds(cleanText(input))\n\nlet inputLength = testIds.length + 2;\n\nlet inputIds = tf.concat([tf.tensor1d([2]), tf.tensor1d(testIds.slice(0, sequenceLength - 2)), tf.tensor1d([3]), tf.zeros([sequenceLength-inputLength])]).reshape([1,sequenceLength]);\n\nlet typeIds = tf.zeros([sequenceLength]).reshape([1,sequenceLength]);\n\nlet inputMask = tf.concat([tf.ones([inputLength]), tf.zeros([sequenceLength-inputLength])]).reshape([1,sequenceLength]);\n\ninputIds.toString()"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["The following cell computes the predictions and outputs the intent with the highest probabilty. You can try different input sentences and see whether the classifications make sense."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>The following cell computes the predictions and outputs the intent with the highest probabilty. You can try different input sentences and see whether the classifications make sense.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let prediction = model.predict([inputIds, typeIds, inputMask])\n\nlet intent = prediction.argMax(1).bufferSync().get(0)\n\nintentsFile[intent]"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["Finally you can look at the confidence with which the model made the classification."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>Finally you can look at the confidence with which the model made the classification.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["prediction.slice([0,intent],[1,1]).toString()"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]}]}
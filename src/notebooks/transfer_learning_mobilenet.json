{"nbformat":4,"nbformat_minor":0,"metadata":{"kernel_info":{"name":"Weblab"},"language_info":{"name":"javascript"}},"cells":[{"cell_type":"markdown","metadata":{},"source":["# Transfer learning for image classification\n\nIn this example we want to train a neural network to distinguish between pictures of cats and pictures of dogs. This task is called image classification and typically requires large neural networks and a lot of training data. Training such large models requires high performance computers and can not be done on a personal computer. Luckily, the training process can be simplyfied by the technique called **transfer learning**. For transfer learning the knowledge from a neural network that was trained on a similar problem is \"transered\" to the actual problem. A similar problem means that the network was trained with a similar objective and on data that shares certain features with the desired dataset.\n  \nFor this problem we will use the pre-trained MobileNet v2 model that has been trained for general image classification on the ImageNet database.\nThe front part of the MobileNet network extracts features from the images and the last two layers are used to deduce the classification based on the given features.\nFor the task of classifying cats and dogs, we will reuse the front part of the network and only retrain the classification part based on our classes.\n\nBe aware that even though transfer learning reduces the complexity of the training process, this example requires substantial computing power and at least 12 GB of memory.\n\nAs a first step, let us import all libraries that we need for machine learning, unziping and image decoding."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<h1>Transfer learning for image classification</h1>\n<p>In this example we want to train a neural network to distinguish between pictures of cats and pictures of dogs. This task is called image classification and typically requires large neural networks and a lot of training data. Training such large models requires high performance computers and can not be done on a personal computer. Luckily, the training process can be simplyfied by the technique called <strong>transfer learning</strong>. For transfer learning the knowledge from a neural network that was trained on a similar problem is &quot;transered&quot; to the actual problem. A similar problem means that the network was trained with a similar objective and on data that shares certain features with the desired dataset.</p>\n<p>For this problem we will use the pre-trained MobileNet v2 model that has been trained for general image classification on the ImageNet database.\nThe front part of the MobileNet network extracts features from the images and the last two layers are used to deduce the classification based on the given features.\nFor the task of classifying cats and dogs, we will reuse the front part of the network and only retrain the classification part based on our classes.</p>\n<p>Be aware that even though transfer learning reduces the complexity of the training process, this example requires substantial computing power and at least 12 GB of memory.</p>\n<p>As a first step, let us import all libraries that we need for machine learning, unziping and image decoding.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["import * as fflate from 'fflate';\nimport * as jpegJs from 'jpeg-js';\nimport * as tf from '@tensorflow/tfjs';\nimport {default as plotly} from \"plotly.js-cartesian-esm\";\n\ntf.version.tfjs"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["## Download and unzip the training images\n\nTo train the pre-trained model for the classification of cats and dogs, we need pictures of cats and dogs as training data. So in the following step we will download a zipped directory with images and store its data as an Arraybuffer."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<h2>Download and unzip the training images</h2>\n<p>To train the pre-trained model for the classification of cats and dogs, we need pictures of cats and dogs as training data. So in the following step we will download a zipped directory with images and store its data as an Arraybuffer.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let compressed = new Uint8Array(await fetch('https://storage.googleapis.com/weblab_datasets/cats_and_dogs_filtered.zip').then(\n  response => response.arrayBuffer()\n));"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["We use the `fflate` library to decompress the zipped data into a Map that contains the local paths of the images and the corresponding data."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>We use the <code>fflate</code> library to decompress the zipped data into a Map that contains the local paths of the images and the corresponding data.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let decompressed = fflate.unzipSync(compressed,{\n  filter: (file) => {\n    // Only decompress images\n    file.name.endsWith('.jpg');\n  }\n});"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["For the training process we need to attach labels to the training data. And hence we need to seperate the decompressed data into arrays of cat images, dog images and validation images."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>For the training process we need to attach labels to the training data. And hence we need to seperate the decompressed data into arrays of cat images, dog images and validation images.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let cats = Object.entries(decompressed).filter(\n  entry => entry[0].startsWith(\"cats_and_dogs_filtered/train/cats\")\n).map(\n  entry => entry[1]\n);\n\nlet dogs = Object.entries(decompressed).filter(\n  entry => entry[0].startsWith(\"cats_and_dogs_filtered/train/dogs\")\n).map(\n  entry => entry[1]\n);\n\nlet cats_validation = Object.entries(decompressed).filter(\n  entry => entry[0].startsWith(\"cats_and_dogs_filtered/validation/cats\")\n).map(\n  entry => entry[1]\n);\n\nlet dogs_validation = Object.entries(decompressed).filter(\n  entry => entry[0].startsWith(\"cats_and_dogs_filtered/validation/dogs\")\n).map(\n  entry => entry[1]\n);"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["## Decode JPEG images and convert them to tensors\n\nIn the next step we will decode the images into bitmap representations and store them inside of tensors. The tensors have 3 dimensions, 2 for the dimensions of the image and 1 for the 3 values of the RGB code. To simplify the classification we resize the images to 224 by 224 pixels. \n\nIn the end we return an object that contains the resized Tensors as its `xs` property and its label as its `ys` property. The labels are tensors with the length equal to the number of classes (in our case 2). The first entry contains the probability of the input being a cat and the second entry the probability of the input being a dog. For the cat images the labels are `[1,0]`. Aferwards, we repeat this step for the dog and validation images.\n\nDuring this process a lot of memory has to be allocated. It is therefore important to perform this tasks in one function call, so that the intermediate results are discarded. Additionally we will wrap these steps inside of a `tf.tidy()` call to make sure that unneeded tensors are cleared. Be aware that executing the following cells can take a while and can lead to temporarily freezing the browser tab."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<h2>Decode JPEG images and convert them to tensors</h2>\n<p>In the next step we will decode the images into bitmap representations and store them inside of tensors. The tensors have 3 dimensions, 2 for the dimensions of the image and 1 for the 3 values of the RGB code. To simplify the classification we resize the images to 224 by 224 pixels.</p>\n<p>In the end we return an object that contains the resized Tensors as its <code>xs</code> property and its label as its <code>ys</code> property. The labels are tensors with the length equal to the number of classes (in our case 2). The first entry contains the probability of the input being a cat and the second entry the probability of the input being a dog. For the cat images the labels are <code>[1,0]</code>. Aferwards, we repeat this step for the dog and validation images.</p>\n<p>During this process a lot of memory has to be allocated. It is therefore important to perform this tasks in one function call, so that the intermediate results are discarded. Additionally we will wrap these steps inside of a <code>tf.tidy()</code> call to make sure that unneeded tensors are cleared. Be aware that executing the following cells can take a while and can lead to temporarily freezing the browser tab.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let cat_tensors = cats.map(\n  buffer => {\n    tf.tidy(\n      () => {\n        let image = jpegJs.decode(buffer, {formatAsRGBA: false, useTArray: true});\n        let tensor = tf.tensor3d(image.data, [image.height, image.width, 3])\n        let resizedTensor = tf.image.resizeNearestNeighbor(tensor,[224,224]\n                                                          ).div(tf.scalar(127.5)).sub(tf.scalar(1.0));\n    \treturn {xs: resizedTensor, ys: tf.tensor1d([1.0, 0.0])}\n      }\n    )\n  }\n);\n\"Done\""],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let dog_tensors = dogs.map(\n  buffer => {\n    tf.tidy(\n      () => {\n        let image = jpegJs.decode(buffer, {formatAsRGBA: false, useTArray: true});\n        let tensor = tf.tensor3d(image.data, [image.height, image.width, 3])\n        let resizedTensor = tf.image.resizeNearestNeighbor(tensor,[224,224]\n                                                          ).div(tf.scalar(127.5)).sub(tf.scalar(1.0));\n    \treturn {xs: resizedTensor, ys: tf.tensor1d([0.0, 1.0])}\n      }\n    )\n  }\n);\n\"Done\""],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let cat_validation_tensors = cats_validation.map(\n  buffer => {\n    tf.tidy(\n      () => {\n        let image = jpegJs.decode(buffer, {formatAsRGBA: false, useTArray: true});\n        let tensor = tf.tensor3d(image.data, [image.height, image.width, 3])\n        let resizedTensor = tf.image.resizeNearestNeighbor(tensor,[224,224]\n                                                          ).div(tf.scalar(127.5)).sub(tf.scalar(1.0));\n    \treturn {xs: resizedTensor, ys: tf.tensor1d([1.0, 0.0])}\n      }\n    )\n  }\n);\n\"Done\""],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let dog_validation_tensors = dogs_validation.map(\n  buffer => {\n    tf.tidy(\n      () => {\n        let image = jpegJs.decode(buffer, {formatAsRGBA: false, useTArray: true});\n        let tensor = tf.tensor3d(image.data, [image.height, image.width, 3])\n        let resizedTensor = tf.image.resizeNearestNeighbor(tensor,[224,224]\n                                                          ).div(tf.scalar(127.5)).sub(tf.scalar(1.0));\n    \treturn {xs: resizedTensor, ys: tf.tensor1d([0.0, 1.0])}\n      }\n    )\n  }\n);\n\"Done\""],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["To ensure an unbiased training process we need to shuffle our training data. Otherwise the model will be trianed with cat images first and then with dof images. To do this conveniently we define a dataset from our arrays of images. We can then easily shuffle and batch the data accordingly."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>To ensure an unbiased training process we need to shuffle our training data. Otherwise the model will be trianed with cat images first and then with dof images. To do this conveniently we define a dataset from our arrays of images. We can then easily shuffle and batch the data accordingly.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let dataset = tf.data.array(cat_tensors.concat(dog_tensors)).shuffle(2000).batch(16);\nlet dataset_validation = tf.data.array( cat_validation_tensors.concat(dog_validation_tensors)).shuffle(1000).batch(8);\n\"Done\""],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["## Create model for transfer learning\n\nIn the next step we setup the neural network for transfer learning. As discussed before the first part of the network is responsible for the feature extraction of the images while the last two layers perform the classification. For the first part we download the \"headless\" version of the MobileNet model, meaning without the classification layers."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<h2>Create model for transfer learning</h2>\n<p>In the next step we setup the neural network for transfer learning. As discussed before the first part of the network is responsible for the feature extraction of the images while the last two layers perform the classification. For the first part we download the &quot;headless&quot; version of the MobileNet model, meaning without the classification layers.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let mobilenet = await tf.loadLayersModel(\"https://storage.googleapis.com/weblab_models/imagenet_mobilenet_v2_100_224_5_feature_vector_tfjs/model.json\");"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["Since we only want to train the classification layers we set the layers of the first part to not be trainable."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>Since we only want to train the classification layers we set the layers of the first part to not be trainable.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["for (layer of mobilenet.layers) {\n  layer.trainable = false\n}"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["Next, we have to define the two classification layers oursevles. The average pooling layer downscales the output of the first part into the feature vector that contains the information about the features present in the current image. The second layer outputs the actual classification. Since we have *2* classes, its `units` parameter is equal to *2*."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>Next, we have to define the two classification layers oursevles. The average pooling layer downscales the output of the first part into the feature vector that contains the information about the features present in the current image. The second layer outputs the actual classification. Since we have <em>2</em> classes, its <code>units</code> parameter is equal to <em>2</em>.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let averagePooling = tf.layers.globalAveragePooling2d({dataFormat: 'channelsLast'}).apply(mobilenet.output);\nlet classification = tf.layers.dense({units: 2, activation: 'sigmoid'}).apply(averagePooling);"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["With our layers in place we can create the model."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>With our layers in place we can create the model.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let model = tf.model({inputs: mobilenet.input, outputs: classification});"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["Let's print out a summary of the model to the console."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>Let's print out a summary of the model to the console.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["model.summary()"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["Finally, we have to specify wich loss function we want to minimize and what optimization algorithm we want to use."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>Finally, we have to specify wich loss function we want to minimize and what optimization algorithm we want to use.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["model.compile({\n  optimizer: 'adam',\n  loss: 'categoricalCrossentropy',\n  metrics: ['accuracy']\n});"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["## Train model\n\nNow that the model structure is defined and the data is setup, it's time to train the model. Running it for only '10' epochs will still give a high enough accuracy."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<h2>Train model</h2>\n<p>Now that the model structure is defined and the data is setup, it's time to train the model. Running it for only '10' epochs will still give a high enough accuracy.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let monitor = document.createElement(\"div\")\nmonitor.id = \"monitor\"\nmonitor"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let lossData = {x: [], y: [], type: \"scatter\", mode: \"lines\"}\n\nplotly.newPlot(\"monitor\",[lossData], {xaxis: {title: \"Epochs\"}, yaxis: {title: \"Loss function\"}})"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["await model.fitDataset(dataset,{epochs: 10, \n                          validationData: dataset_validation, \n                          callbacks: [{onEpochEnd: (epoch, logs) => {\n                              lossData.x.push(epoch)\n                              lossData.y.push(logs.loss)\n                              plotly.redraw(\"monitor\")\n                              return console.log(epoch, logs.loss)\n                          }}]\n                        });"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["## Make prediction\n\nLet us test if the transfer learning worked by classifying a picture from a remote url. For that we load the picture into an image element. If you have the url of a different picture, you can try that instead."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<h2>Make prediction</h2>\n<p>Let us test if the transfer learning worked by classifying a picture from a remote url. For that we load the picture into an image element. If you have the url of a different picture, you can try that instead.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let img = document.createElement(\"img\");\nimg.height = 224\nimg.width = 224\nimg.src = 'https://i.imgur.com/JlUvsxa.jpg'\nimg.crossOrigin = 'anonymous'\nimg"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]},{"cell_type":"markdown","metadata":{},"source":["We will then convert the image element into a tensor and pass it to the prediction function of our model. Let's see what the prediction is."],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":"<p>We will then convert the image element into a tensor and pass it to the prediction function of our model. Let's see what the prediction is.</p>\n"}}]},{"cell_type":"code","execution_count":0,"metadata":{},"source":["let labels = [\"cat\", \"dog\"];\n\nlet imgTensor = tf.browser.fromPixels(img).reshape([1,224,224,3]);\n\nlet prediction = model.predict(imgTensor).argMax(1).arraySync()[0];\n\nlabels[prediction]"],"outputs":[{"output_type":"display_data","metadata":{},"data":{"text/plain":""}}]}]}